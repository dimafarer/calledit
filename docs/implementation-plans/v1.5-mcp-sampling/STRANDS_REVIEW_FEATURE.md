# Strands Turn-Based Review & Improvement Feature

## Overview
After generating an initial prediction call, Strands automatically reviews its own response and identifies parts that could be improved with additional user information. Users can click on highlighted sections to provide more context and regenerate improved responses.

## Feature Flow

### Phase 1: Initial Call Generation
1. User submits prediction via streaming WebSocket
2. Strands generates initial response with all standard fields:
   - `prediction_statement`
   - `verification_date` 
   - `verifiable_category`
   - `category_reasoning`
   - `verification_method`
3. **Initial response displayed immediately** (no waiting for review)

### Phase 2: Automatic Review
1. Strands automatically reviews each part of its response
2. For each reviewable section, Strands determines:
   - Could this be improved with more user info?
   - What specific questions would help?
   - Could this change verifiability category?
3. Review results streamed to frontend via WebSocket
4. UI updates to highlight improvable sections

### Phase 3: User Interaction
1. **Visual Highlighting**: Improvable sections get obvious mobile-friendly highlighting
2. **Individual Clickable Elements**: Each highlighted section is independently clickable
3. **Specific Questions**: Clicking triggers Strands to ask targeted questions
4. **Real-time Streaming**: All interactions happen over WebSocket with live updates

### Phase 4: Regeneration
1. User provides additional information
2. Strands regenerates response with new context
3. **Smart Updates**:
   - Significant changes â†’ Replace entire response
   - Minor changes â†’ Update only affected sections
4. **Re-review**: Updated response goes through review process again
5. **Version Tracking**: Store original + improved versions with improvement reasons

## Data Structure

### Review Data
```json
{
  "review_id": "uuid",
  "original_response": { /* full original response */ },
  "reviewable_sections": [
    {
      "section": "prediction_statement",
      "improvable": true,
      "questions": ["What specific time of day?", "Any location constraints?"],
      "reasoning": "More temporal precision could change verifiability category"
    },
    {
      "section": "verification_method", 
      "improvable": true,
      "questions": ["Do you have access to specific data sources?"],
      "reasoning": "User's data access could enable better verification approach"
    }
  ],
  "improvements": [
    {
      "section": "prediction_statement",
      "original_value": "Bitcoin will hit $100k today",
      "improved_value": "Bitcoin will hit $100k before 3pm EST today",
      "user_input": "I meant before 3pm Eastern time",
      "improvement_reasoning": "Added temporal precision"
    }
  ]
}
```

## UI/UX Requirements

### Mobile-First Highlighting
- **High contrast borders** (not just background colors)
- **Touch-friendly click targets** (minimum 44px)
- **Clear visual indicators** (icons, badges, or borders)
- **Accessible color schemes** (not relying only on color)

### Interaction Patterns
- **Progressive disclosure**: Show questions only when section clicked
- **Loading states**: Show streaming indicators during regeneration
- **Version comparison**: Option to see before/after changes

## Technical Implementation

### WebSocket Message Types
```json
// Initial response
{"type": "call_response", "data": { /* standard response */ }}

// Review results
{"type": "review_complete", "data": { "reviewable_sections": [...] }}

// User requests improvement
{"type": "improve_section", "data": {"section": "prediction_statement"}}

// Strands asks questions
{"type": "improvement_questions", "data": {"questions": [...], "section": "..."}}

// User provides answers
{"type": "improvement_answers", "data": {"answers": [...], "section": "..."}}

// Regenerated response
{"type": "improved_response", "data": { /* updated response */ }}
```

### Database Schema
- Extend existing call records with `review_data` field
- Track improvement history and reasoning
- Store both original and final versions

## Current Status

**Phase 1**: âœ… Initial call generation working perfectly (100% success rate)
**Phase 2**: âœ… Automatic review working (returns `review_complete` messages)
**Phase 3**: âœ… User interaction working (WebSocket routing fixed, mock responses working)
**Phase 4**: ðŸ”„ Regeneration ready (needs full ReviewAgent implementation)

## Implementation Status

### âœ… **MCP Sampling Pattern**: Correctly implemented following MCP specification
- **Server-controlled**: Strands automatically reviews its response
- **Client-facilitated**: WebSocket client handles LLM sampling requests  
- **Human-in-the-loop**: User approval and input required for improvements
- **Multi-step workflow**: Review â†’ Questions â†’ User input â†’ Regeneration

### âœ… **WebSocket Infrastructure**: Complete routing for improvement workflow
- `improve_section` route: âœ… Working (mock responses received)
- `improvement_answers` route: âœ… Ready for implementation
- Lambda imports: âœ… Fixed (relative â†’ absolute imports)
- Permissions: âœ… Proper WebSocket API permissions configured

### ðŸ”„ **Next Steps**: Replace mock with full ReviewAgent functionality
- Implement actual ReviewAgent calls instead of mock responses
- Add frontend UI components for improvement workflow
- Store improvement history in DynamoDB

## Success Metrics
- **Engagement**: % of users who click on highlighted sections
- **Improvement Quality**: Verifiability category upgrades (human â†’ tool)
- **User Satisfaction**: Reduced need for manual call editing
- **Performance**: Review completion time < 5 seconds

## Future Enhancements
- **Learning**: Strands learns common improvement patterns
- **Proactive Suggestions**: Suggest improvements before user asks
- **Batch Improvements**: Improve multiple sections simultaneously